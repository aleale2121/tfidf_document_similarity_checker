{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b8d841",
   "metadata": {},
   "source": [
    "# DOCUMENT SIMILARITY "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44623774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26422f94",
   "metadata": {},
   "source": [
    "## STEP 1. DATA PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e84dca",
   "metadata": {},
   "source": [
    "### 1.1 Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ba61e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FILE NAMES\n",
      "['doc6.txt', 'doc1.txt', 'doc7.txt', 'doc4.txt', 'doc2.txt', 'doc5.txt', 'doc3.txt']\n",
      "\n",
      "\n",
      "FILE PATHS\n",
      "['./data/doc6.txt', './data/doc1.txt', './data/doc7.txt', './data/doc4.txt', './data/doc2.txt', './data/doc5.txt', './data/doc3.txt']\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"./data/\"\n",
    "\n",
    "def getFileInformations(folderPath):\n",
    "    fileInformations = []\n",
    "    listOfFileNames = [fileName for fileName in listdir(folderPath) if isfile(join(folderPath, fileName))]\n",
    "    listOfFilePaths = [join(folderPath, fileName) for fileName in listdir(folderPath) if isfile(join(folderPath, fileName))]\n",
    "    fileInformations.append(listOfFileNames)\n",
    "    fileInformations.append(listOfFilePaths)\n",
    "    return fileInformations\n",
    "fileNames, filePaths = getFileInformations(DATA_DIR)\n",
    "print(\"FILE NAMES\")\n",
    "print(fileNames)\n",
    "print(\"\\n\")\n",
    "print(\"FILE PATHS\")\n",
    "print(filePaths)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e95a85d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'./data/doc6.txt': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor fine ut labore et tura magna hjhd. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris lash ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat time pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n', './data/doc1.txt': 'Every other Saturday, Master Rick of the Seattle Zen Monastery asked me to sweep the courtyard with a brittle straw broom. This week was more than the usual load: pigeons had left their excrement over the courtyard–which meant I had to mop up their mess after sweeping. Despite this extra work, the words of Master Rick–a tall, straw-thin man in his 60s–echoed in my mind: “Sweep like you are sweeping your thoughts away–one at a time.” I wasn’t sure if it was a mystical statement or just an excuse to get a 17-year-old to do his chores. But it didn’t matter what I thought: I had to do my duty and that was the end of it. \\n', './data/doc7.txt': 'Awol ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor fine ut labore et tura magna hjhd. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris lash ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat time pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n', './data/doc4.txt': 'Every other Saturday, Master Rick of the Seattle Zen Monastery asked me to sweep the courtyard with a brittle straw broom. This week was more than the usual load: pigeons had left their excrement over the courtyard–which meant I had to mop up their mess after sweeping. Despite this extra work, the words of Master Rick–a tall, straw-thin man in his 60s–echoed in my mind: “Sweep like you are sweeping your thoughts away–one at a time.” I wasn’t sure if it was a mystical statement or just an excuse to get a 17-year-old to do his chores. But it didn’t matter what I thought: I had to do my duty and that was the end of it. \\n', './data/doc2.txt': 'Every other Saturday, Master Jhon of the Seattle Zen Monastery asked me to sweep the courtyard with a brittle straw broom. This week was more than the usual load: pigeons had left their excrement over the courtyard–which meant I had to mop up their mess after sweeping. Despite this extra work, the words of Master Rick–a tall, straw-thin man in his 60s–echoed in my mind: “Sweep like you are like your happens away–one at a time.” I wasn’t sure if it was a mystical statement or just an excuse to get a 17-year-old to do his chores. But it didn’t sacks what I ever: I had to do my he who that was the end of it. ETAOIN SHRDLU\\n', './data/doc5.txt': 'The quick brown fox jumps over the lazy dog', './data/doc3.txt': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n'}\n"
     ]
    }
   ],
   "source": [
    "def create_docContentDict(filePaths):\n",
    "    rawContentDict = {}\n",
    "    for filePath in filePaths:\n",
    "        with open(filePath, \"r\") as ifile:\n",
    "            fileContent = ifile.read()\n",
    "        rawContentDict[filePath] = fileContent\n",
    "    return rawContentDict\n",
    "rawContentDict = create_docContentDict(filePaths)\n",
    "print(rawContentDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0a15f",
   "metadata": {},
   "source": [
    "### 1.2 TOKENIZATION OF CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43186d2",
   "metadata": {},
   "source": [
    "#### 1.2.1 Tokenize\n",
    "\n",
    "Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f354df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokenizedData(rawData):\n",
    "    return nltk.tokenize.word_tokenize(rawData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae39372a",
   "metadata": {},
   "source": [
    "#### 1.2.2 Remove Stop words\n",
    "\n",
    "Stopwords are the English words which does not add much meaning to a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bffaab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "804c2ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(rawData):\n",
    "    stop_word_set = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    return  [word for word in rawData if word not in stop_word_set] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7b2bd",
   "metadata": {},
   "source": [
    "#### 1.2.3 Stemming\n",
    "\n",
    "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d34bdd",
   "metadata": {},
   "source": [
    "Porter’s Stemmer algorithm \n",
    "\n",
    "It is based on the idea that the suffixes in the English language are made up of a combination of smaller and simpler suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500074f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performPorterStemming(rawData):\n",
    "    porterStemmer = nltk.stem.PorterStemmer()\n",
    "    return [porterStemmer.stem(word) for word in rawData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ee863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSpace(rawDate):\n",
    "    return [word.strip() for word in rawData]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847a8c4",
   "metadata": {},
   "source": [
    "#### 1.2.4 Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1043fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuations(rawData):\n",
    "    excludePuncuation = set(string.punctuation)\n",
    "    excludePuncuation.update(['\\'\\'','--','``'])\n",
    "    return [word for word in rawData if word not in excludePuncuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaba7cf0",
   "metadata": {},
   "source": [
    "#### 1.2.5 Convert To Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a35daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertItemsToLower(rawData):\n",
    "    return  [term.lower() for term in rawData] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750e6c1",
   "metadata": {},
   "source": [
    "#### 1.2.6 Summerize Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a850c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(rawContents):\n",
    "    cleaned = getTokenizedData(rawContents)\n",
    "    cleaned = removeStopWords(cleaned)\n",
    "    cleaned = performPorterStemming(cleaned)    \n",
    "    cleaned = removePunctuations(cleaned)\n",
    "    cleaned = convertItemsToLower(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8ba8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everi', 'saturday', 'master', 'rick', 'seattl', 'zen', 'monasteri', 'ask', 'sweep', 'courtyard', 'brittl', 'straw', 'broom', 'thi', 'week', 'usual', 'load', 'pigeon', 'left', 'excrement', 'courtyard–which', 'meant', 'i', 'mop', 'mess', 'sweep', 'despit', 'extra', 'work', 'word', 'master', 'rick–a', 'tall', 'straw-thin', 'man', '60s–echo', 'mind', '“', 'sweep', 'like', 'sweep', 'thought', 'away–on', 'time.', '”', 'i', '’', 'sure', 'mystic', 'statement', 'excus', 'get', '17-year-old', 'chore', 'but', '’', 'matter', 'i', 'thought', 'i', 'duti', 'end']\n"
     ]
    }
   ],
   "source": [
    "a=cleanData(rawContentDict[filePaths[1]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22d48a",
   "metadata": {},
   "source": [
    "## STEP 2 :PROCESS SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4436a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             doc1.txt  doc2.txt  doc3.txt  doc4.txt  doc5.txt  doc6.txt  \\\n",
      "17-year-old  0.000000  0.115026  0.000000  0.115026  0.117907       0.0   \n",
      "60s–echo     0.000000  0.115026  0.000000  0.115026  0.117907       0.0   \n",
      "ad           0.110966  0.000000  0.110546  0.000000  0.000000       0.0   \n",
      "adipisc      0.110966  0.000000  0.110546  0.000000  0.000000       0.0   \n",
      "aliqua       0.000000  0.000000  0.000000  0.000000  0.000000       0.0   \n",
      "...               ...       ...       ...       ...       ...       ...   \n",
      "work         0.000000  0.115026  0.000000  0.115026  0.117907       0.0   \n",
      "zen          0.000000  0.115026  0.000000  0.115026  0.117907       0.0   \n",
      "’            0.000000  0.230051  0.000000  0.230051  0.235814       0.0   \n",
      "“            0.000000  0.115026  0.000000  0.115026  0.117907       0.0   \n",
      "”            0.000000  0.115026  0.000000  0.115026  0.117907       0.0   \n",
      "\n",
      "             doc7.txt  \n",
      "17-year-old  0.000000  \n",
      "60s–echo     0.000000  \n",
      "ad           0.105813  \n",
      "adipisc      0.105813  \n",
      "aliqua       0.149131  \n",
      "...               ...  \n",
      "work         0.000000  \n",
      "zen          0.000000  \n",
      "’            0.000000  \n",
      "“            0.000000  \n",
      "”            0.000000  \n",
      "\n",
      "[125 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=cleanData, stop_words='english')\n",
    "tfs = tfidf.fit_transform(rawContentDict.values())\n",
    "#print(\"--------------tf Values---------------------------------\")\n",
    "tfs_Values = tfs.toarray()\n",
    "#print(tfs_Values)\n",
    "#print(\"--------------tfs Terms     ----------------------------\")\n",
    "tfs_Term = tfidf.get_feature_names()\n",
    "#print(tfs_Term)\n",
    "\n",
    "skDocsTfIdfdf = pd.DataFrame(tfs.todense(),index=sorted(fileNames),  columns=tfs_Term)\n",
    "print(skDocsTfIdfdf.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6435c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          doc1.txt  doc2.txt  doc3.txt  doc4.txt  doc5.txt  doc6.txt  doc7.txt\n",
      "doc1.txt  1.000000  0.000000  0.979429  0.000000  0.000000       0.0  0.908437\n",
      "doc2.txt  0.000000  1.000000  0.000000  1.000000  0.854426       0.0  0.000000\n",
      "doc3.txt  0.979429  0.000000  1.000000  0.000000  0.000000       0.0  0.888992\n",
      "doc4.txt  0.000000  1.000000  0.000000  1.000000  0.854426       0.0  0.000000\n",
      "doc5.txt  0.000000  0.854426  0.000000  0.854426  1.000000       0.0  0.000000\n",
      "doc6.txt  0.000000  0.000000  0.000000  0.000000  0.000000       1.0  0.000000\n",
      "doc7.txt  0.908437  0.000000  0.888992  0.000000  0.000000       0.0  1.000000\n"
     ]
    }
   ],
   "source": [
    "#compute cosine similarity\n",
    "csim = cosine_similarity(tfs,tfs)\n",
    "csimDf = pd.DataFrame(csim,index=sorted(fileNames),columns=sorted(fileNames))\n",
    "print(csimDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f4d03a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
